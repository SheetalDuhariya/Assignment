{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yigRweK18M-T"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "                                                            Assignment Questions\n",
        "\n",
        "\n",
        "1. What is a parameter?\n",
        "\n",
        "In feature engineering, a parameter refers to the internal variables or values that a machine learning model adjusts during training to optimize its performance.\n",
        "These parameters are not part of the raw data but are learned by the model to define how features contribute to predictions.\n",
        "\n",
        ".................................\n",
        "2. What is correlation?\n",
        "\n",
        "In feature engineering, correlation measures the statistical association between two variables, helping to understand how features interact and influence each other.\n",
        "It is particularly useful for feature selection, model interpretability, and addressing multicollinearity.\n",
        "\n",
        "...................................\n",
        "3. What does negative correlation mean?\n",
        "\n",
        "In feature engineering, negative correlation refers to an inverse relationship between two variables, where one variable increases as the other decreases.\n",
        "This concept is important when analyzing relationships between features or between features and the target variable.\n",
        "\n",
        "......................................\n",
        "4. Deﬁne Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "Machine Learning (ML) is a subset of Artificial Intelligence (AI) that enables computers to learn from data and improve their performance over time without being explicitly programmed.\n",
        "It involves training algorithms to identify patterns, make predictions, and make decisions based on data.\n",
        "\n",
        "Main Components of Machine Learning:\n",
        "\n",
        "Machine learning algorithms consist of three primary components that define how they function\n",
        "\n",
        ":\n",
        "\n",
        "    Representation:\n",
        "\n",
        "        This refers to how knowledge or the problem is represented in the model. Examples include:\n",
        "\n",
        "            Decision trees\n",
        "\n",
        "            Neural networks\n",
        "\n",
        "            Support vector machines (SVMs)\n",
        "\n",
        "            Graphical models\n",
        "\n",
        "            Ensembles of models\n",
        "\n",
        "        The choice of representation determines the structure and capability of the model.\n",
        "\n",
        "    Evaluation:\n",
        "\n",
        "        Evaluation measures how well a model performs on a given task or dataset. Common metrics include:\n",
        "\n",
        "            Accuracy\n",
        "\n",
        "            Precision and recall\n",
        "\n",
        "            Squared error\n",
        "\n",
        "            Likelihood or posterior probability\n",
        "\n",
        "        These metrics help assess the quality of predictions and guide improvements.\n",
        "\n",
        "    Optimization:\n",
        "\n",
        "        Optimization involves finding the best solution or parameters for the model by minimizing or maximizing an objective function (e.g., error rate, cost).\n",
        "\n",
        "        Techniques include:\n",
        "\n",
        "            Gradient descent\n",
        "\n",
        "            Convex optimization\n",
        "\n",
        "            Combinatorial optimization\n",
        "\n",
        "\n",
        "........................................\n",
        "5. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "The loss value in machine learning is a critical metric used to evaluate how well a model is performing by quantifying the error between the model's predictions and the actual target values.\n",
        "It helps determine whether the model is good or not based on its ability to minimize this error.\n",
        "\n",
        ".................................................\n",
        "6. What are continuous and categorical variables?\n",
        "\n",
        "Feature\t                                Continuous Variable\t                    Categorical Variable\n",
        "Nature\t                                Measurable quantities\t                  Descriptive categories\n",
        "Possible Values\t                        Infinite within a range\t                Finite set of categories\n",
        "Examples\t                              Height, weight, time\t                  Hair color, pizza type\n",
        "Subtypes\t                              Interval and ratio scales\t              Nominal and ordinal\n",
        "\n",
        "\n",
        "........................................................\n",
        "7. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "Technique\t                                Use Case\tA                           dvantages\t                            Limitations\n",
        "One-Hot Encoding\t              Few unique categories\t                    Simple and interpretable\t            Increases dimensionality\n",
        "Label Encoding\t                Ordinal or tree-based models\t            Compact representation\t              Implies ordinal relationships\n",
        "Dummy Encoding\t                Avoiding redundancy\t                      Reduces multicollinearity\t            Similar to One-Hot Encoding\n",
        "Target Encoding\t                High-cardinality features\t                Captures relationship with target   \tRisk of overfitting\n",
        "Frequency/Count\t                High-cardinality features\t                Reduces dimensionality\t              Loses detailed information\n",
        "\n",
        ".....................................................\n",
        "8. What do you mean by training and testing a dataset?\n",
        "\n",
        "In machine learning, training and testing datasets are essential components used to build and evaluate models.\n",
        "They serve distinct purposes in the development process to ensure the model learns effectively and performs well on unseen data.\n",
        "\n",
        "....................................................\n",
        "9. What is sklearn.preprocessing?\n",
        "\n",
        "The sklearn.preprocessing module in Scikit-learn provides a suite of tools and transformer classes for preparing raw data to be more suitable for machine learning algorithms. Many machine learning models perform better when the input data is standardized, normalized, or transformed into a specific format.\n",
        "This module automates these preprocessing tasks, ensuring the data is ready for downstream estimators.\n",
        "\n",
        ".................................................\n",
        "10. What is a Test set?\n",
        "\n",
        "A test set is a subset of data used to evaluate the performance of a trained machine learning model.\n",
        "It is kept separate from the training and validation datasets to ensure an unbiased assessment of how well the model generalizes to new, unseen data.\n",
        "\n",
        "..........................................................................\n",
        "11. How do we split data for model ﬁtting (training and testing) in Python?\n",
        "\n",
        "To split data for model fitting (training and testing) in Python, the most common method is to use the train_test_split function from the scikit-learn library. This function allows you to divide a dataset into training and testing subsets, ensuring that the model is trained on one portion of the data and evaluated on another.\n",
        "Steps to Split Data Using train_test_split:\n",
        "\n",
        "    Import Required Libraries:\n",
        "\n",
        "python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Prepare Your Dataset:\n",
        "\n",
        "    Separate your dataset into features (X) and target variable (y).\n",
        "\n",
        "    Example:\n",
        "\n",
        "    python\n",
        "    X = dataset.iloc[:, :-1]  # Features (all columns except the last)\n",
        "    y = dataset.iloc[:, -1]   # Target variable (last column)\n",
        "\n",
        "Split the Data:\n",
        "Use train_test_split() to divide the data into training and testing sets.\n",
        "\n",
        "python\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    test_size: Proportion of the dataset to be used as the test set (e.g., 0.2 for 20%).\n",
        "\n",
        "    random_state: Ensures reproducibility by controlling random shuffling of data.\n",
        "\n",
        "Check Shapes of Splits:\n",
        "Verify that the data has been split correctly.\n",
        "\n",
        "python\n",
        "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n",
        "\n",
        "\n",
        "........................................................\n",
        "12. How do you approach a Machine Learning problem?\n",
        "\n",
        "Step                            \tDescription\n",
        "Understand Problem              \tDefine objectives and success criteria\n",
        "Collect Data\t                    Gather relevant and representative data\n",
        "Preprocess Data\t                  Clean, transform, and engineer features\n",
        "Explore Data\t                    Visualize patterns and relationships\n",
        "Choose Model\t                    Select algorithm based on problem type\n",
        "Split Data\t                      Divide into training/testing sets\n",
        "Train Model\t                      Fit model to training data\n",
        "Evaluate Model\t                  Assess performance on test data\n",
        "Iterate\t                          Refine preprocessing or try new models\n",
        "Deploy\t                          Implement model in productio\n",
        "..................................................................\n",
        "13. Why do we have to perform EDA before ﬁtting a model to the data?\n",
        "\n",
        "Performing Exploratory Data Analysis (EDA) before fitting a machine learning model is essential for several reasons. EDA helps ensure the quality of the data, identify patterns, and prepare it for modeling, ultimately improving the model's accuracy and reliability.\n",
        "Why EDA is Necessary Before Fitting a Model\n",
        "\n",
        "    Understanding the Data:\n",
        "\n",
        "        EDA helps you familiarize yourself with the dataset, including its structure, types of variables, distributions, and relationships between features\n",
        "\n",
        "    .\n",
        "\n",
        "    It allows you to identify which variables are relevant for modeling and which may need transformation or removal.\n",
        "\n",
        "Identifying Patterns and Relationships:\n",
        "\n",
        "    EDA uncovers trends, correlations, and interactions between features and the target variable. This insight can guide feature selection and engineering\n",
        "\n",
        "    .\n",
        "\n",
        "Detecting Anomalies:\n",
        "\n",
        "    Outliers, missing values, duplicates, or inconsistencies can negatively impact model performance. EDA helps detect and address these issues early\n",
        "\n",
        "    .\n",
        "\n",
        "Choosing Appropriate Models:\n",
        "\n",
        "    By analyzing data distributions and relationships, EDA can help determine whether certain statistical techniques or machine learning algorithms are suitable for the problem\n",
        "\n",
        "    .\n",
        "\n",
        "Improving Model Accuracy:\n",
        "\n",
        "    Cleaning and transforming data based on insights from EDA ensures that models are trained on high-quality inputs, reducing errors and improving predictions\n",
        "\n",
        "    .\n",
        "\n",
        "Avoiding Overfitting:\n",
        "\n",
        "    Identifying skewed distributions or irrelevant features during EDA prevents overfitting by ensuring the model focuses on meaningful data\n",
        "\n",
        "        .\n",
        "\n",
        "Common Techniques in EDA\n",
        "\n",
        "    Visualization: Histograms, scatter plots, box plots, pair plots to explore distributions and relationships.\n",
        "\n",
        "    Statistical Analysis: Descriptive statistics (mean, median, variance), correlation matrices.\n",
        "\n",
        "    Data Cleaning: Handling missing values, removing duplicates/outliers.\n",
        "\n",
        "    Feature Engineering: Creating new features or transforming existing ones based on insights\n",
        "\n",
        "............................................................\n",
        "15. How can you ﬁnd correlation between variables in Python?\n",
        "\n",
        "Correlation measures the strength and direction of a relationship between two variables. In Python, there are several methods to calculate correlation, depending\n",
        " on the type of data and the desired correlation coefficient (e.g., Pearson, Spearman, or Kendall). Below are the most common approaches:\n",
        "\n",
        "...........................................................................................\n",
        "16. What is causation? Explain difference between correlation and causation with an example\n",
        "\n",
        "Causation refers to a relationship where one variable directly causes a change in another. In other words, changes in one variable (the cause) lead to changes in another variable (the effect).\n",
        "Establishing causation requires rigorous testing and evidence, such as controlled experiments, to rule out other possible explanations.\n",
        "\n",
        "Aspect\t                                  Correlation\t                                                                                             Causation\n",
        "Definition                A statistical measure that shows whether two variables move together (positively or negatively).                       \tA direct cause-and-effect relationship between two variables.\n",
        "Implication\t              Indicates association but not necessarily a causal link.\t                                                              Implies that one variable directly influences the other.\n",
        "Proof Requirement\t        Requires statistical analysis (e.g., correlation coefficient).\t                                                        Requires experimental evidence with controlled variables to establish causality.\n",
        "Example\t                  Ice cream sales and shark attacks are correlated because both increase in summer.                                     \tSmoking causes lung cancer, as proven through controlled studies.\n",
        "\n",
        "..............................................................................................\n",
        "17. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "An optimizer is an algorithm used in machine learning to adjust the parameters (e.g., weights and biases) of a model during training to minimize the loss function. The goal of the optimizer is to find the optimal set of parameters that result in the lowest possible error between the predicted outputs and the actual target values. It plays a critical role in ensuring that the model learns effectively and converges to a solution.\n",
        "\n",
        "Optimizers work by iteratively updating the model's parameters using gradients computed through backpropagation. They help navigate the high-dimensional parameter space efficiently, avoiding local minima and ensuring convergence.\n",
        "Types of Optimizers in Machine Learning\n",
        "\n",
        "Here are some of the most commonly used optimizers, along with explanations and examples:\n",
        "1. Gradient Descent (GD)\n",
        "\n",
        "    Description: Gradient Descent is the simplest optimization algorithm. It updates parameters by moving in the direction opposite to the gradient of the loss function with respect to the parameters.\n",
        "\n",
        "    Formula:\n",
        "    θ=θ−α∇J(θ)\n",
        "    θ=θ−α∇J(θ)\n",
        "\n",
        "    where:\n",
        "\n",
        "        θθ: Model parameters\n",
        "\n",
        "        αα: Learning rate\n",
        "\n",
        "        ∇J(θ)∇J(θ): Gradient of the loss function\n",
        "\n",
        "    Example: Used for small-scale problems where computing gradients over the entire dataset is feasible.\n",
        "\n",
        "    Limitation: Computationally expensive for large datasets.\n",
        "\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "    Description: Instead of computing gradients over the entire dataset, SGD updates parameters using one data point at a time, making it faster but noisier.\n",
        "\n",
        "    Formula:\n",
        "    θ=θ−α∇J(θ;x(i),y(i))\n",
        "    θ=θ−α∇J(θ;x(i),y(i))\n",
        "\n",
        "    where x(i)x(i) and y(i)y(i) are individual training examples.\n",
        "\n",
        "    Example: Commonly used in deep learning tasks like image classification.\n",
        "\n",
        "    Limitation: High variance in updates can lead to instability.\n",
        "\n",
        "3. Mini-Batch Gradient Descent\n",
        "\n",
        "    Description: A compromise between GD and SGD, it computes gradients on small batches of data instead of the entire dataset or a single data point.\n",
        "\n",
        "    Example: Widely used in deep learning frameworks like TensorFlow and PyTorch for efficient training.\n",
        "\n",
        "4. Momentum\n",
        "\n",
        "    Description: Momentum accelerates gradient descent by adding a fraction of the previous update to the current update, helping overcome local minima and speeding up convergence.\n",
        "\n",
        "    Formula:\n",
        "    vt=βvt−1+(1−β)∇J(θ)\n",
        "    vt=βvt−1+(1−β)∇J(θ)\n",
        "\n",
        "    where vtvt is the velocity term.\n",
        "\n",
        "    Example: Often combined with SGD to improve convergence speed.\n",
        "\n",
        "5. RMSProp\n",
        "\n",
        "    Description: RMSProp adjusts learning rates for each parameter by dividing by a moving average of recent gradient magnitudes, making it effective for non-stationary objectives.\n",
        "\n",
        "    Formula:\n",
        "    vt=βvt−1+(1−β)(∇J(θ))2\n",
        "    vt=βvt−1+(1−β)(∇J(θ))2\n",
        "\n",
        "    Parameters are updated as:\n",
        "    θ=θ−αvt+ϵ(∇J(θ))\n",
        "    θ=θ−vt+ϵ\n",
        "\n",
        "    α(∇J(θ))\n",
        "\n",
        "    Example: Commonly used for recurrent neural networks (RNNs).\n",
        "\n",
        "6. Adam (Adaptive Moment Estimation)\n",
        "\n",
        "    Description: Combines momentum and RMSProp by maintaining moving averages of both gradients and squared gradients.\n",
        "\n",
        "    Formula:\n",
        "\n",
        "        First moment estimate:\n",
        "        mt=β1mt−1+(1−β1)∇J(θ)\n",
        "        mt=β1mt−1+(1−β1)∇J(θ)\n",
        "\n",
        "        Second moment estimate:\n",
        "        vt=β2vt−1+(1−β2)(∇J(θ))2\n",
        "        vt=β2vt−1+(1−β2)(∇J(θ))2\n",
        "\n",
        "    Parameters are updated as:\n",
        "    θ=θ−αv^t+ϵ(m^t)\n",
        "    θ=θ−v^t\n",
        "\n",
        "    +ϵα(m^t)\n",
        "\n",
        "    Example: Widely used across various deep learning applications due to its efficiency and adaptability.\n",
        "\n",
        "    Strengths: Handles sparse gradients well and works with large datasets.\n",
        "\n",
        "7. Adagrad\n",
        "\n",
        "    Description: Adapts learning rates for each parameter based on past gradients, making it effective for sparse data.\n",
        "\n",
        "    Formula:\n",
        "    Parameters are updated as:\n",
        "    gt=gt−1+(∇J(θ))2\n",
        "    gt=gt−1+(∇J(θ))2\n",
        "\n",
        "    Update rule:\n",
        "    θ=θ−αgt+ϵ(∇J(θ))\n",
        "    θ=θ−gt+ϵ\n",
        "\n",
        "    α(∇J(θ))\n",
        "\n",
        "    Example: Effective for text or image data with sparse features.\n",
        "\n",
        "    Limitation: Learning rates can shrink too much over time.\n",
        "\n",
        "8. AdaDelta\n",
        "\n",
        "    Description: An improvement over Adagrad that limits shrinking learning rates by using a decaying average of past squared gradients.\n",
        "\n",
        "    Strengths: No need to manually set a learning rate.\n",
        "\n",
        "9. Nesterov Accelerated Gradient (NAG)\n",
        "\n",
        "    Description: A variant of Momentum that looks ahead before updating parameters, reducing overshooting issues.\n",
        "\n",
        "    Formula:\n",
        "    Parameters are updated as:\n",
        "    vt=mvt−1−(∇J(θ+mvt))\n",
        "    vt=mvt−1−(∇J(θ+mvt))\n",
        "\n",
        ".....................................\n",
        "18. What is sklearn.linear_model ?\n",
        "\n",
        "sklearn.linear_model is a module in the Scikit-learn library that provides a collection of linear models for regression and classification tasks. These models are based on the assumption that the target variable is a linear combination of the input features.\n",
        "The module includes simple linear regression, logistic regression, and more advanced techniques like Ridge, Lasso, and ElasticNet.\n",
        "\n",
        ".........................................................\n",
        "19. What does model.ﬁt() do? What arguments must be given?\n",
        "\n",
        "The fit() method in Scikit-learn is used to train a machine learning model. It adjusts the model's internal parameters (e.g., weights, biases) by learning from the training data. The goal is to minimize the error between the predicted and actual target values by finding patterns in the data.\n",
        "\n",
        "When you call model.fit(X, y):\n",
        "\n",
        "    The model learns the relationship between the input features (X) and the target variable (y).\n",
        "\n",
        "    The parameters of the model (e.g., coefficients in linear regression or decision boundaries in classification) are optimized based on the data.\n",
        "\n",
        "    Once trained, the model object stores these learned parameters, which can then be used for making predictions on new data.\n",
        "\n",
        "Arguments Required by fit()\n",
        "\n",
        "The fit() method typically requires two arguments:\n",
        "\n",
        "    X (Features):\n",
        "\n",
        "        A 2D array-like structure (e.g., NumPy array, Pandas DataFrame) containing the input features.\n",
        "\n",
        "        Each row represents a sample, and each column represents a feature.\n",
        "\n",
        "        Example: For a dataset with 100 samples and 3 features, X will have a shape of (100, 3).\n",
        "\n",
        "    y (Target Variable):\n",
        "\n",
        "        A 1D array-like structure containing the target values (labels) corresponding to each sample in X.\n",
        "\n",
        "        For regression tasks, y contains continuous values.\n",
        "\n",
        "        For classification tasks, y contains categorical labels.\n",
        "\n",
        "\n",
        "..............................................................\n",
        "20. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "The predict() method in Scikit-learn is used to make predictions on new, unseen data after a machine learning model has been trained using the fit() method. It takes input data (features) and outputs predicted values based on the patterns the model has learned during training.\n",
        "\n",
        "    For classification models, it predicts the class labels for the input data.\n",
        "\n",
        "    For regression models, it predicts continuous values.\n",
        "\n",
        "Arguments Required by predict()\n",
        "\n",
        "    X (Input Features):\n",
        "\n",
        "        A 2D array-like structure (e.g., NumPy array, Pandas DataFrame) containing the input features for which predictions are to be made.\n",
        "\n",
        "        The shape of X must match the number of features used during training.\n",
        "\n",
        "How predict() Works\n",
        "\n",
        "    The model uses the parameters it learned during training (e.g., weights and biases in linear regression, decision boundaries in classification) to compute predictions.\n",
        "\n",
        "    The input data is passed through the model's internal logic to generate outputs.\n",
        "\n",
        "....................................................................\n",
        "22. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "Feature scaling is a technique used in machine learning to standardize the range of independent variables or features of the data. In many machine learning algorithms, the scale of features can significantly impact model performance and convergence speed. Feature scaling ensures that all features contribute equally to the distance calculations used in algorithms like k-nearest neighbors (KNN), support vector machines (SVM), and gradient descent optimization.\n",
        "Common Methods of Feature Scaling\n",
        "\n",
        "    Min-Max Scaling (Normalization):\n",
        "\n",
        "        Transforms features to a fixed range, usually.\n",
        "\n",
        "        Formula:\n",
        "        X′=X−XminXmax−Xmin\n",
        "        X′=Xmax−XminX−Xmin\n",
        "\n",
        "        Example: If a feature has values ranging from 10 to 50, after min-max scaling, these values will be transformed to fall between 0 and 1.\n",
        "\n",
        "    Standardization (Z-score Normalization):\n",
        "\n",
        "        Centers the feature by subtracting the mean and scales it by dividing by the standard deviation.\n",
        "\n",
        "        Formula:\n",
        "        X′=X−μσ\n",
        "        X′=σX−μ\n",
        "\n",
        "        where μμ is the mean and σσ is the standard deviation.\n",
        "\n",
        "        Example: A feature with a mean of 100 and a standard deviation of 15 will have its values transformed to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "    Robust Scaling:\n",
        "\n",
        "        Uses the median and interquartile range for scaling, making it robust to outliers.\n",
        "\n",
        "        Formula:\n",
        "        X′=X−Q1Q3−Q1\n",
        "        X′=Q3−Q1X−Q1\n",
        "\n",
        "        where Q1Q1 and Q3Q3 are the first and third quartiles.\n",
        "\n",
        "        Example: This method is useful when dealing with datasets that contain significant outliers.\n",
        "\n",
        "How Feature Scaling Helps in Machine Learning\n",
        "\n",
        "    Improves Convergence Speed:\n",
        "\n",
        "        Algorithms that rely on gradient descent (e.g., linear regression, logistic regression) converge faster when features are on a similar scale. This reduces the number of iterations needed for convergence.\n",
        "\n",
        "    Enhances Model Performance:\n",
        "\n",
        "        Many algorithms, especially distance-based ones like KNN and clustering algorithms, are sensitive to the scale of data. Features with larger ranges can dominate distance calculations, leading to biased results.\n",
        "\n",
        "    Facilitates Better Interpretability:\n",
        "\n",
        "        When features are scaled to a common range, it becomes easier to interpret model coefficients or feature importances since they are on a comparable scale.\n",
        "\n",
        "    Prevents Numerical Instability:\n",
        "\n",
        "        Scaling can help avoid numerical instability issues in optimization algorithms, especially when dealing with very large or very small values.\n",
        "\n",
        "    Ensures Fair Contribution:\n",
        "\n",
        "        In models that combine multiple features (e.g., linear models), scaling ensures that each feature contributes equally to the final prediction.\n",
        "\n",
        "When to Use Feature Scaling\n",
        "\n",
        "    Use feature scaling when:\n",
        "\n",
        "        The dataset contains features with different units or scales.\n",
        "\n",
        "        You are using algorithms sensitive to feature scales (e.g., KNN, SVM).\n",
        "\n",
        "        You want to improve convergence speed in gradient descent-based models.\n",
        "\n",
        "In summary, feature scaling is a crucial preprocessing step in machine learning that enhances model performance and\n",
        "interpretability by ensuring that all features contribute equally during training and prediction.\n",
        "\n",
        "........................................\n",
        "23. How do we perform scaling in Python?\n",
        "\n",
        "Scaling data in Python can be done using the sklearn.preprocessing module from Scikit-learn. Below are common scaling techniques and their implementations:\n",
        "1. Standardization\n",
        "\n",
        "Standardization transforms features so they have a mean of 0 and a standard deviation of 1. It is useful when features have different units or ranges.\n",
        "Formula:\n",
        "z=x−μσ\n",
        "z=σx−μ\n",
        "\n",
        "Where:\n",
        "\n",
        "    xx: Original value\n",
        "\n",
        "    μμ: Mean of the feature\n",
        "\n",
        "    σσ: Standard deviation of the feature\n",
        "\n",
        "................................\n",
        "26. Explain data encoding?\n",
        "Data encoding is the process of converting categorical variables into numerical representations so that they can be understood and processed by machine learning algorithms. Since most machine learning models work with numerical data,\n",
        "categorical data (e.g., labels like \"red,\" \"green,\" or \"blue\") must be transformed into a suitable format.\n",
        "\n",
        "\n",
        "'''\n"
      ]
    }
  ]
}